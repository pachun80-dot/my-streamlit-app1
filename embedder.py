import hashlib
import json
import os
import pickle
import re
import time

import numpy as np
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

# ëª¨ë“ˆ ë ˆë²¨ ìºì‹œ: ëª¨ë¸ì„ í•œ ë²ˆë§Œ ë¡œë“œ
_model = None

# ì„ë² ë”© ìºì‹œ ì €ì¥ í´ë”
_CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), ".embedding_cache")


def _get_model() -> SentenceTransformer:
    """ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸ì„ ë¡œë“œí•œë‹¤ (ì‹±ê¸€í„´)."""
    global _model
    if _model is None:
        # íƒ€ì„ì•„ì›ƒ ì„¤ì • (ëª¨ë¸ ë‹¤ìš´ë¡œë“œìš© - ìµœì´ˆ 1íšŒë§Œ)
        import os
        os.environ['HF_HUB_TIMEOUT'] = '300'  # 5ë¶„
        _model = SentenceTransformer("intfloat/multilingual-e5-large")
    return _model


def _prepare_text(text: str, is_query: bool = False) -> str:
    """E5 ëª¨ë¸ ì…ë ¥ í˜•ì‹ì— ë§ê²Œ ì ‘ë‘ì‚¬ë¥¼ ì¶”ê°€í•œë‹¤."""
    prefix = "query: " if is_query else "passage: "
    return prefix + text.strip()


def _make_cache_key(korea_articles: list[dict]) -> str:
    """í•œêµ­ë²• ì¡°ë¬¸ ëª©ë¡ìœ¼ë¡œë¶€í„° ìºì‹œ í‚¤(í•´ì‹œ)ë¥¼ ìƒì„±í•œë‹¤."""
    content = json.dumps(
        [{"id": a["id"], "text": a["text"], "source": a.get("source", "")}
         for a in korea_articles],
        ensure_ascii=False,
        sort_keys=True,
    )
    return hashlib.sha256(content.encode("utf-8")).hexdigest()[:16]


def _load_cache(cache_key: str) -> dict | None:
    """ìºì‹œ íŒŒì¼ì´ ì¡´ì¬í•˜ë©´ ë¡œë“œí•œë‹¤."""
    cache_path = os.path.join(_CACHE_DIR, f"{cache_key}.pkl")
    if os.path.exists(cache_path):
        with open(cache_path, "rb") as f:
            return pickle.load(f)
    return None


def _save_cache(cache_key: str, index: dict) -> None:
    """ì„ë² ë”© ì¸ë±ìŠ¤ë¥¼ ìºì‹œ íŒŒì¼ë¡œ ì €ì¥í•œë‹¤."""
    os.makedirs(_CACHE_DIR, exist_ok=True)
    cache_path = os.path.join(_CACHE_DIR, f"{cache_key}.pkl")
    with open(cache_path, "wb") as f:
        pickle.dump(index, f)


def build_korea_index(korea_articles: list[dict], use_cache: bool = True) -> dict:
    """í•œêµ­ë²• ì¡°ë¬¸ ì„ë² ë”© ì¸ë±ìŠ¤ë¥¼ êµ¬ì¶•í•œë‹¤.

    Args:
        korea_articles: í•œêµ­ë²• ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸
        use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€ (êµ¬ì¡°í™” ì—‘ì…€ì€ ê³„ì† ìˆ˜ì •ë˜ë¯€ë¡œ False ê¶Œì¥)

    ê°™ì€ í•œêµ­ë²• ì¡°í•©ì´ë©´ ìºì‹œì—ì„œ ë¶ˆëŸ¬ì˜¤ê³ ,
    ì²˜ìŒì´ë©´ ì„ë² ë”© í›„ ìºì‹œì— ì €ì¥í•œë‹¤.
    """
    cache_key = _make_cache_key(korea_articles)

    if use_cache:
        cached = _load_cache(cache_key)
        if cached is not None:
            return cached

    model = _get_model()
    texts = [_prepare_text(a["text"]) for a in korea_articles]
    embeddings = model.encode(texts, show_progress_bar=True, normalize_embeddings=True)
    index = {
        "articles": korea_articles,
        "embeddings": np.array(embeddings),
    }

    if use_cache:
        _save_cache(cache_key, index)
    return index


def find_similar_korean(
    foreign_article: dict,
    korea_index: dict,
    top_k: int = 1,
) -> list[dict]:
    """ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ ì¡°ë¬¸ ê²€ìƒ‰ (í´ë°±ìš©)."""
    if not korea_index["articles"]:
        return []

    model = _get_model()
    query_text = _prepare_text(foreign_article["text"], is_query=True)
    query_embedding = model.encode([query_text], normalize_embeddings=True)

    scores = cosine_similarity(query_embedding, korea_index["embeddings"])[0]
    top_indices = np.argsort(scores)[::-1][:top_k]

    results = []
    for idx in top_indices:
        article = korea_index["articles"][idx]
        results.append({
            "korean_id": article["id"],
            "korean_text": article["text"],
            "score": float(scores[idx]),
            "source": article.get("source", ""),
        })
    return results


# â”€â”€ AI ê¸°ë°˜ ë§¤ì¹­ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _call_gemini(prompt: str, system: str, max_retries: int = 3) -> str:
    """Gemini APIë¥¼ ì¬ì‹œë„ í¬í•¨í•˜ì—¬ í˜¸ì¶œí•œë‹¤."""
    import streamlit as st
    import google.generativeai as genai

    api_key = st.secrets.get("GEMINI_API_KEY", "")
    if not api_key or api_key == "your-key-here":
        return ""

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel("gemini-2.5-flash", system_instruction=system)

    for attempt in range(max_retries):
        try:
            response = model.generate_content(
                prompt,
                request_options={"timeout": 120},
            )
            time.sleep(1)
            return response.text.strip()
        except Exception:
            if attempt < max_retries - 1:
                time.sleep(3 * (attempt + 1))
            else:
                return ""


def _call_claude(prompt: str, system: str, max_retries: int = 3) -> str:
    """Claude APIë¥¼ ì¬ì‹œë„ í¬í•¨í•˜ì—¬ í˜¸ì¶œí•œë‹¤."""
    import streamlit as st
    import anthropic

    api_key = st.secrets.get("ANTHROPIC_API_KEY", "")
    if not api_key or api_key == "your-key-here":
        return ""

    for attempt in range(max_retries):
        try:
            client = anthropic.Anthropic(api_key=api_key)
            message = client.messages.create(
                model="claude-sonnet-4-5-20250929",
                max_tokens=2048,
                system=system,
                messages=[{"role": "user", "content": prompt}],
            )
            time.sleep(1)
            return message.content[0].text.strip()
        except Exception:
            if attempt < max_retries - 1:
                time.sleep(3 * (attempt + 1))
            else:
                return ""


def select_relevant_korean_laws(
    foreign_law_name: str,
    foreign_sample_text: str,
    korea_law_names: list[str],
) -> list[str]:
    """[1ë‹¨ê³„] AIê°€ í•´ì™¸ ë²•ë ¹ì˜ ì£¼ì œë¥¼ ë³´ê³  ê´€ë ¨ í•œêµ­ë²•ì„ ì„ íƒí•œë‹¤.

    Args:
        foreign_law_name: í•´ì™¸ ë²•ë ¹ íŒŒì¼ëª…
        foreign_sample_text: í•´ì™¸ ë²•ë ¹ ì•ë¶€ë¶„ ìƒ˜í”Œ (ë²ˆì—­ë¬¸)
        korea_law_names: í•œêµ­ë²• íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸

    Returns:
        ê´€ë ¨ í•œêµ­ë²• íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸ (1~2ê°œ)
    """
    law_list = "\n".join(f"{i+1}. {name}" for i, name in enumerate(korea_law_names))

    prompt = (
        f"í•´ì™¸ ë²•ë ¹: {foreign_law_name}\n\n"
        f"í•´ì™¸ ë²•ë ¹ ë‚´ìš© ìƒ˜í”Œ (ë²ˆì—­ë¬¸):\n{foreign_sample_text[:1000]}\n\n"
        f"ì•„ë˜ í•œêµ­ ë²•ë ¹ ëª©ë¡ ì¤‘ì—ì„œ ìœ„ í•´ì™¸ ë²•ë ¹ê³¼ ê·œìœ¨ ë¶„ì•¼ê°€ ê°€ì¥ ê´€ë ¨ ìˆëŠ” "
        f"í•œêµ­ ë²•ë ¹ì„ 1~2ê°œ ì„ íƒí•˜ì‹­ì‹œì˜¤.\n\n"
        f"{law_list}\n\n"
        f"ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤ (ë²ˆí˜¸ë§Œ):\n"
        f"ì„ íƒ: 1, 3"
    )

    answer = _call_gemini(
        prompt,
        "ë‹¹ì‹ ì€ í•œêµ­ ì§€ì‹ì¬ì‚°ê¶Œë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•´ì™¸ ë²•ë ¹ì˜ ê·œìœ¨ ë¶„ì•¼ë¥¼ íŒŒì•…í•˜ê³  "
        "ê°€ì¥ ê´€ë ¨ ìˆëŠ” í•œêµ­ ë²•ë ¹ì„ ì •í™•íˆ ì„ íƒí•©ë‹ˆë‹¤."
    )

    if not answer:
        return korea_law_names  # ì‹¤íŒ¨ ì‹œ ì „ì²´ ë°˜í™˜

    # ë²ˆí˜¸ íŒŒì‹±
    selected = []
    for line in answer.split("\n"):
        if "ì„ íƒ" in line or any(c.isdigit() for c in line):
            nums = re.findall(r"\d+", line)
            for n in nums:
                idx = int(n) - 1
                if 0 <= idx < len(korea_law_names):
                    selected.append(korea_law_names[idx])

    return selected if selected else korea_law_names


def match_article_with_korean_law(
    translated_text: str,
    foreign_article_id: str,
    korean_articles: list[dict],
    korean_law_name: str,
    foreign_article_title: str = "",
) -> dict | None:
    """[2ë‹¨ê³„] AIê°€ í•´ì™¸ë²• ë²ˆì—­ë¬¸ì„ ì½ê³  í•œêµ­ë²• ì¡°ë¬¸ ëª©ë¡ì—ì„œ ë§¤ì¹­í•œë‹¤.

    Args:
        translated_text: í•´ì™¸ë²• ë²ˆì—­ë¬¸ (í•œêµ­ì–´)
        foreign_article_id: í•´ì™¸ë²• ì¡°ë¬¸ ë²ˆí˜¸
        korean_articles: í•´ë‹¹ í•œêµ­ë²•ì˜ ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ [{'id':..., 'text':...}, ...]
        korean_law_name: í•œêµ­ë²• íŒŒì¼ëª…
        foreign_article_title: í•´ì™¸ë²• ì¡°ë¬¸ ì œëª© (ìˆëŠ” ê²½ìš°)

    Returns:
        {'korean_id', 'korean_text', 'score', 'source', 'ai_reason'} or None
    """
    if not korean_articles:
        return None

    # ì‘ë‹µ íŒŒì‹± í—¬í¼ í•¨ìˆ˜
    def parse_ai_response(answer):
        """AI ì‘ë‹µì—ì„œ ì„ íƒê³¼ ì´ìœ  ì¶”ì¶œ"""
        if not answer or "ì—†ìŒ" in answer:
            return None, ""
        chosen_id = None
        ai_reason = ""
        for line in answer.split("\n"):
            line = line.strip()
            if line.startswith("ì„ íƒ:"):
                text = line.replace("ì„ íƒ:", "").strip().strip("[]").strip()
                if text and text != "ì—†ìŒ":
                    chosen_id = text
            elif line.startswith("ì´ìœ :"):
                ai_reason = line.replace("ì´ìœ :", "").strip()
        return chosen_id, ai_reason

    # ì¡°ë¬¸ ì°¾ê¸° í—¬í¼ í•¨ìˆ˜
    def find_korean_article(chosen_id, korean_articles):
        """ì„ íƒëœ ì¡°ë¬¸ IDë¡œ í•œêµ­ë²• ì¡°ë¬¸ ì°¾ê¸°"""
        if not chosen_id:
            return None
        # ì •í™•íˆ ì¼ì¹˜
        for a in korean_articles:
            if a["id"] == chosen_id:
                return a
        # ë¶€ë¶„ ë§¤ì¹­
        for a in korean_articles:
            if chosen_id in a["id"] or a["id"] in chosen_id:
                return a
        # ìˆ«ìë§Œ ì¶”ì¶œí•´ì„œ ë¹„êµ
        chosen_nums = re.findall(r"\d+", chosen_id)
        for a in korean_articles:
            article_nums = re.findall(r"\d+", a["id"])
            if chosen_nums and article_nums and chosen_nums[0] == article_nums[0]:
                return a
        return None

    # 1ë‹¨ê³„: ì¡°ë¬¸ ì œëª© ê¸°ë°˜ AI ë§¤ì¹­ (Gemini + Claude)
    if foreign_article_title and foreign_article_title.strip():
        # í•œêµ­ë²• ì¡°ë¬¸ ì œëª© ëª©ë¡ êµ¬ì„±
        title_list = ""
        for a in korean_articles:
            if a["id"] == "ì „ë¬¸":
                continue
            korean_title = a.get("title", "").strip()
            if korean_title:
                title_list += f"- {a['id']}: {korean_title}\n"

        # ì œëª©ì´ ìˆëŠ” í•œêµ­ë²• ì¡°ë¬¸ì´ ìˆìœ¼ë©´ AIë¡œ ì œëª© ë§¤ì¹­
        if title_list:
            title_prompt = (
                f"ì™¸êµ­ë²• ì¡°ë¬¸ ì œëª©: '{foreign_article_title}'\n\n"
                f"ì•„ë˜ í•œêµ­ ë²•ë ¹({korean_law_name})ì˜ ì¡°ë¬¸ ì œëª© ëª©ë¡ì—ì„œ "
                f"ìœ„ ì™¸êµ­ë²• ì¡°ë¬¸ ì œëª©ê³¼ ì˜ë¯¸ì ìœ¼ë¡œ ë™ì¼í•˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ì¡°ë¬¸ì„ 1ê°œë§Œ ì„ íƒí•˜ì‹­ì‹œì˜¤.\n"
                f"ì˜ë¯¸ê°€ ëª…í™•íˆ ë‹¤ë¥´ê±°ë‚˜ ìœ ì‚¬í•œ ì¡°ë¬¸ì´ ì—†ìœ¼ë©´ ë°˜ë“œì‹œ 'ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì‹­ì‹œì˜¤.\n\n"
                f"{title_list}\n"
                f"ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:\n"
                f"ì„ íƒ: [ì¡°ë¬¸ë²ˆí˜¸] (ë˜ëŠ” 'ì—†ìŒ')\n"
                f"ì´ìœ : [1ë¬¸ì¥ ì´ìœ ]"
            )

            title_system = (
                "ë‹¹ì‹ ì€ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¡°ë¬¸ ì œëª©ì˜ ì˜ë¯¸ë¥¼ ì •í™•íˆ ë¹„êµí•˜ì—¬ "
                "ë™ì¼í•˜ê±°ë‚˜ ë§¤ìš° ìœ ì‚¬í•œ ê²½ìš°ë§Œ ë§¤ì¹­í•˜ì‹­ì‹œì˜¤. "
                "ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ì˜ë¯¸ê°€ ë‹¤ë¥´ë©´ ë°˜ë“œì‹œ 'ì—†ìŒ'ìœ¼ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤."
            )

            # Geminiì™€ Claude ë™ì‹œ í˜¸ì¶œ
            gemini_answer = _call_gemini(title_prompt, title_system)
            claude_answer = _call_claude(title_prompt, title_system)

            gemini_id, gemini_reason = parse_ai_response(gemini_answer)
            claude_id, claude_reason = parse_ai_response(claude_answer)

            # ë‘ AIê°€ ê°™ì€ ì¡°ë¬¸ì„ ì„ íƒí•œ ê²½ìš°ë§Œ ë§¤ì¹­
            if gemini_id and claude_id and gemini_id == claude_id:
                article = find_korean_article(gemini_id, korean_articles)
                if article:
                    return {
                        "korean_id": article["id"],
                        "korean_text": article["text"],
                        "score": 1.0,
                        "source": article.get("source", korean_law_name),
                        "ai_reason": f"[ì œëª© ë§¤ì¹­ - ì–‘ìª½ AI ì¼ì¹˜] {gemini_reason or claude_reason}",
                    }

    # 2ë‹¨ê³„: ì¡°ë¬¸ ë‚´ìš© ê¸°ë°˜ AI ë§¤ì¹­ (Gemini + Claude)
    # í•œêµ­ë²• ì¡°ë¬¸ ëª©ë¡ êµ¬ì„± (ì¡°ë¬¸ë²ˆí˜¸ + ì• 150ì)
    article_list = ""
    for a in korean_articles:
        if a["id"] == "ì „ë¬¸":
            continue
        summary = a["text"][:150].replace("\n", " ")
        article_list += f"- {a['id']}: {summary}\n"

    content_prompt = (
        f"í•´ì™¸ ë²•ë ¹ ì¡°ë¬¸ ({foreign_article_id}) ë²ˆì—­ë¬¸:\n"
        f"{translated_text[:500]}\n\n"  # ë„ˆë¬´ ê¸¸ë©´ ì˜ë¼ì„œ ì „ë‹¬
        f"ì•„ë˜ í•œêµ­ ë²•ë ¹({korean_law_name})ì˜ ì¡°ë¬¸ ëª©ë¡ì—ì„œ "
        f"ìœ„ í•´ì™¸ë²• ì¡°ë¬¸ê³¼ ê·œìœ¨ ë‚´ìš©ì´ ê°€ì¥ ìœ ì‚¬í•œ ì¡°ë¬¸ì„ 1ê°œ ì„ íƒí•˜ì‹­ì‹œì˜¤.\n"
        f"ìœ ì‚¬í•œ ì¡°ë¬¸ì´ ì „í˜€ ì—†ìœ¼ë©´ 'ì—†ìŒ'ì´ë¼ê³  ë‹µí•˜ì‹­ì‹œì˜¤.\n\n"
        f"{article_list}\n"
        f"ë°˜ë“œì‹œ ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œë§Œ ë‹µí•˜ì‹­ì‹œì˜¤:\n"
        f"ì„ íƒ: [ì¡°ë¬¸ë²ˆí˜¸] (ë˜ëŠ” 'ì—†ìŒ')\n"
        f"ì´ìœ : [1ë¬¸ì¥ ì´ìœ ]"
    )

    content_system = (
        "ë‹¹ì‹ ì€ í•œêµ­ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. í•´ì™¸ ë²•ë ¹ ì¡°ë¬¸ì˜ ê·œìœ¨ ë‚´ìš©ì„ ì •í™•íˆ íŒŒì•…í•˜ê³ , "
        "í•œêµ­ ë²•ë ¹ì—ì„œ ë™ì¼í•˜ê±°ë‚˜ ê°€ì¥ ìœ ì‚¬í•œ ë‚´ìš©ì„ ê·œìœ¨í•˜ëŠ” ì¡°ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤. "
        "ì¡°ë¬¸ ë²ˆí˜¸ê°€ ê°™ë‹¤ê³  ë‚´ìš©ì´ ê°™ì€ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. ë°˜ë“œì‹œ ë‚´ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•˜ì‹­ì‹œì˜¤. "
        "ë¶ˆí™•ì‹¤í•˜ê±°ë‚˜ ìœ ì‚¬í•œ ì¡°ë¬¸ì´ ì—†ìœ¼ë©´ 'ì—†ìŒ'ìœ¼ë¡œ ë‹µí•˜ì‹­ì‹œì˜¤."
    )

    # Geminiì™€ Claude ë™ì‹œ í˜¸ì¶œ
    gemini_answer = _call_gemini(content_prompt, content_system)
    claude_answer = _call_claude(content_prompt, content_system)

    gemini_id, gemini_reason = parse_ai_response(gemini_answer)
    claude_id, claude_reason = parse_ai_response(claude_answer)

    # ë‘ AIê°€ ê°™ì€ ì¡°ë¬¸ì„ ì„ íƒí•œ ê²½ìš°ë§Œ ë§¤ì¹­
    if gemini_id and claude_id and gemini_id == claude_id:
        article = find_korean_article(gemini_id, korean_articles)
        if article:
            return {
                "korean_id": article["id"],
                "korean_text": article["text"],
                "score": 1.0,
                "source": article.get("source", korean_law_name),
                "ai_reason": f"[ë‚´ìš© ë§¤ì¹­ - ì–‘ìª½ AI ì¼ì¹˜] {gemini_reason or claude_reason}",
            }

    # ë‘ AIê°€ ë‹¤ë¥¸ ê²°ê³¼ë¥¼ ë‚¸ ê²½ìš° â†’ ë§¤ì¹­ ì‹¤íŒ¨
    return None


def find_similar_korean_ai(
    foreign_article: dict,
    translated_text: str,
    korea_index: dict,
    relevant_law_sources: list[str] | None = None,
    foreign_article_title: str = "",
) -> list[dict]:
    """AI ê¸°ë°˜ í•œêµ­ë²• ë§¤ì¹­ (2ë‹¨ê³„).

    Args:
        foreign_article: {'id': 'ì¡°ë¬¸ë²ˆí˜¸', 'text': 'ì›ë¬¸', 'ì¡°ë¬¸ì œëª©': 'ì œëª©'(ì„ íƒ)}
        translated_text: í•´ì™¸ë²• ë²ˆì—­ë¬¸ (í•œêµ­ì–´)
        korea_index: build_korea_index()ì˜ ë°˜í™˜ê°’
        relevant_law_sources: 1ë‹¨ê³„ì—ì„œ ì„ íƒëœ ê´€ë ¨ í•œêµ­ë²• íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        foreign_article_title: í•´ì™¸ë²• ì¡°ë¬¸ ì œëª© (ìˆëŠ” ê²½ìš°)

    Returns:
        [{'korean_id', 'korean_text', 'score', 'source', 'ai_reason'}]
    """
    if not korea_index["articles"]:
        return []

    # ê´€ë ¨ í•œêµ­ë²•ë³„ë¡œ ì¡°ë¬¸ ê·¸ë£¹í•‘
    articles_by_law: dict[str, list[dict]] = {}
    for a in korea_index["articles"]:
        src = a.get("source", "")
        if relevant_law_sources and src not in relevant_law_sources:
            continue
        articles_by_law.setdefault(src, []).append(a)

    if not articles_by_law:
        # ê´€ë ¨ë²•ì´ ì—†ìœ¼ë©´ ì „ì²´ì—ì„œ ì‹œë„
        for a in korea_index["articles"]:
            src = a.get("source", "")
            articles_by_law.setdefault(src, []).append(a)

    # ê° ê´€ë ¨ í•œêµ­ë²•ì—ì„œ ë§¤ì¹­ ì‹œë„
    best_match = None
    # ì¡°ë¬¸ ì œëª© ì¶”ì¶œ (ì œê³µë˜ì§€ ì•Šì€ ê²½ìš° foreign_articleì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    if not foreign_article_title and "ì¡°ë¬¸ì œëª©" in foreign_article:
        foreign_article_title = str(foreign_article.get("ì¡°ë¬¸ì œëª©", ""))

    for law_source, articles in articles_by_law.items():
        result = match_article_with_korean_law(
            translated_text,
            foreign_article["id"],
            articles,
            law_source,
            foreign_article_title,
        )
        if result:
            best_match = result
            break  # ì²« ë²ˆì§¸ ë§¤ì¹­ì—ì„œ ì„±ê³µí•˜ë©´ ì¢…ë£Œ

    if best_match:
        return [best_match]

    # AI ë§¤ì¹­ ì‹¤íŒ¨ ì‹œ ë§¤ì¹­ ì—†ìŒ ë°˜í™˜ (ë¬´ì¡°ê±´ ë§¤ì¹­í•  í•„ìš” ì—†ìŒ)
    return []


def _parse_batch_matches(response_text: str, korea_articles: list[dict]) -> dict[str, list[dict]]:
    """AI ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ë§¤ì¹­ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•œë‹¤."""
    # JSON íŒŒì‹±
    if "```json" in response_text:
        json_start = response_text.find("```json") + 7
        json_end = response_text.find("```", json_start)
        json_text = response_text[json_start:json_end].strip()
    else:
        json_text = response_text

    result = json.loads(json_text)
    matches = result.get('matches', [])

    result_dict = {}
    for match in matches:
        foreign_id = str(match.get('foreign_id', ''))
        korean_id = match.get('korean_id')

        if korean_id and korean_id != "null":
            korean_text = ""
            korean_source = ""
            for k_art in korea_articles:
                if str(k_art['id']) == str(korean_id):
                    korean_text = k_art.get('text', '')
                    korean_source = k_art.get('source', '')
                    break

            result_dict[foreign_id] = [{
                'korean_id': str(korean_id),
                'korean_title': match.get('korean_title', ''),
                'korean_text': korean_text,
                'score': float(match.get('score', 0.9)),
                'ai_reason': match.get('reason', ''),
                'source': korean_source
            }]
        else:
            result_dict[foreign_id] = []

    return result_dict


def find_similar_korean_batch(
    foreign_articles: list[dict],
    korea_index: dict,
    relevant_law_sources: list[str] | None = None,
    batch_size: int = 30,
) -> dict[str, list[dict]]:
    """ì™¸êµ­ë²• ì¡°ë¬¸ë“¤ì„ í•œêµ­ë²•ê³¼ ì¼ê´„ ë§¤ì¹­í•œë‹¤.

    ì¡°ë¬¸ ìˆ˜ê°€ ë§ìœ¼ë©´ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•œë‹¤.

    Args:
        foreign_articles: ì™¸êµ­ë²• ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸
            ê° ì¡°ë¬¸ì€ {'id': ì¡°ë¬¸ë²ˆí˜¸, 'text': ì›ë¬¸, 'ì¡°ë¬¸ì œëª©': ì œëª©, 'translated': ë²ˆì—­ë¬¸} í¬í•¨
        korea_index: í•œêµ­ë²• ì¸ë±ìŠ¤ {'articles': [...]}
        relevant_law_sources: ë§¤ì¹­ ëŒ€ìƒ í•œêµ­ë²• í•„í„° (ì˜ˆ: ["íŠ¹í—ˆë²•", "ì‹¤ìš©ì‹ ì•ˆë²•"])
        batch_size: í•œ ë²ˆì— ë§¤ì¹­í•  ì™¸êµ­ë²• ì¡°ë¬¸ ìˆ˜ (ê¸°ë³¸ 30ê°œ)

    Returns:
        ì¡°ë¬¸ IDë¥¼ í‚¤ë¡œ, ë§¤ì¹­ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°’ìœ¼ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬
        ì˜ˆ: {'1': [{'korean_id': '2', 'score': 0.95, ...}], '2': [...], ...}
    """
    import anthropic
    import streamlit as st

    # API í‚¤ í™•ì¸
    api_key = st.secrets.get("ANTHROPIC_API_KEY", "")
    if not api_key:
        print("âŒ ANTHROPIC_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {}

    # í•œêµ­ë²• ì¡°ë¬¸ í•„í„°ë§
    korea_articles = korea_index.get("articles", [])
    if relevant_law_sources:
        korea_articles = [
            art for art in korea_articles
            if art.get("source", "") in relevant_law_sources
        ]

    # í•œêµ­ë²• ì¡°ë¬¸ ë¦¬ìŠ¤íŠ¸ (ê³µí†µ)
    korea_list_str = "\n".join([
        f"ì œ{art['id']}ì¡°: {art.get('title', '')}"
        for art in korea_articles[:100]
    ])

    client = anthropic.Anthropic(api_key=api_key)

    # ë°°ì¹˜ ë¶„í• 
    batches = [
        foreign_articles[i:i + batch_size]
        for i in range(0, len(foreign_articles), batch_size)
    ]

    all_results = {}
    total_batches = len(batches)

    for batch_idx, batch in enumerate(batches):
        if total_batches > 1:
            st.write(f"ğŸ“¦ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch)}ê°œ ì¡°ë¬¸)")

        foreign_list_str = "\n".join([
            f"{art['id']}: {art.get('ì¡°ë¬¸ì œëª©', '')}"
            for art in batch
        ])

        prompt = f"""ë‹¹ì‹ ì€ íŠ¹í—ˆë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì™¸êµ­ íŠ¹í—ˆë²• ì¡°ë¬¸ë“¤ê³¼ í•œêµ­ íŠ¹í—ˆë²• ì¡°ë¬¸ë“¤ì´ ì£¼ì–´ì¡ŒìŠµë‹ˆë‹¤.

**ì™¸êµ­ë²• ì¡°ë¬¸ ì œëª©:**
{foreign_list_str}

**í•œêµ­ íŠ¹í—ˆë²• ì¡°ë¬¸ ì œëª©:**
{korea_list_str}

ê° ì™¸êµ­ë²• ì¡°ë¬¸ì— ëŒ€í•´ ê°€ì¥ ìœ ì‚¬í•œ í•œêµ­ íŠ¹í—ˆë²• ì¡°ë¬¸ì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ì‘ë‹µ í˜•ì‹ (JSON):**
```json
{{
  "matches": [
    {{
      "foreign_id": "1",
      "korean_id": "2",
      "korean_title": "...",
      "score": 0.95,
      "reason": "ë§¤ì¹­ ì´ìœ "
    }},
    ...
  ]
}}
```

ë§¤ì¹­ì´ ì—†ìœ¼ë©´ korean_idë¥¼ nullë¡œ ì„¤ì •í•˜ì„¸ìš”. JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""

        try:
            response_text = ""
            with client.messages.stream(
                model="claude-sonnet-4-20250514",
                max_tokens=16000,
                messages=[{"role": "user", "content": prompt}]
            ) as stream:
                for text in stream.text_stream:
                    response_text += text

            batch_results = _parse_batch_matches(response_text, korea_articles)
            all_results.update(batch_results)

            if total_batches > 1:
                st.write(f"  âœ… ë°°ì¹˜ {batch_idx + 1} ì™„ë£Œ: {len(batch_results)}ê°œ ë§¤ì¹­")

        except Exception as e:
            st.error(f"âŒ ë°°ì¹˜ {batch_idx + 1} ë§¤ì¹­ ì˜¤ë¥˜: {e}")
            st.write("ì‘ë‹µ ë‚´ìš©:", response_text[:500] if 'response_text' in locals() else "ì‘ë‹µ ì—†ìŒ")

        # ë°°ì¹˜ ê°„ ëŒ€ê¸° (rate limit ë°©ì§€)
        if batch_idx < total_batches - 1:
            time.sleep(2)

    st.write(f"ğŸ“Š ìµœì¢… ë§¤ì¹­: {len(all_results)}ê°œ ì¡°ë¬¸")
    return all_results
